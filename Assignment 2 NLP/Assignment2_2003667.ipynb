{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import nltk\n",
    "from nltk import re\n",
    "import en_core_web_sm\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = WordNetLemmatizer()\n",
    "stop_words = STOP_WORDS\n",
    "digits = string.digits\n",
    "global verbose #if the value is 1 - we output all information, if it is 0 - we output only results of average metrics` values\n",
    "verbose=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_word(raw_word,all_words):\n",
    "    word=raw_word.lower()\n",
    "    word=stemmer.lemmatize(word)\n",
    "    if len(word)<=2 or any(map(str.isdigit, word)) or word in stop_words: \n",
    "        return None\n",
    "    else:\n",
    "        all_words.append(word)\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review(review,all_words):\n",
    "    for w in review[:]:\n",
    "        word_res=clean_word(w,all_words)\n",
    "        if word_res!=None:\n",
    "            review[review.index(w)]=word_res\n",
    "        else:\n",
    "            review.remove(w)\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_documents():\n",
    "    documents=[]\n",
    "    all_words=[]\n",
    "    for category in movie_reviews.categories():\n",
    "        for fileid in movie_reviews.fileids(category):\n",
    "            review=list(movie_reviews.words(fileid))\n",
    "            review=clean_review(review,all_words)\n",
    "            documents.append((review, category))\n",
    "    if verbose==1:\n",
    "        print (len(documents))\n",
    "        print(documents[1])\n",
    "    return documents,all_words\n",
    "\n",
    "# In[17]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_common_words(word_features_number,all_words):\n",
    "    all_words = nltk.FreqDist(all_words)\n",
    "    \n",
    "    most_common_words=all_words.most_common(word_features_number)\n",
    "    if verbose==1:\n",
    "        print(len(all_words))\n",
    "        print(*most_common_words[:100], sep='\\n')\n",
    "        print(len(most_common_words))\n",
    "    return most_common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def feature_selector(most_common_words):\n",
    "    nlp = en_core_web_sm.load()\n",
    "    word_features=list()\n",
    "    tags=[\"ADJ\",\"ADP\",\"ADV\",\"AUX\",\"CONJ\",\"CCONJ\",\"DET\",\"INTJ\",\"NOUN\",\"NUM\",\"PART\",\"PRON\",\"PROPN\",\"PUNCT\",\"SCONJ\",\"SYM\",\"VERB\",\"X\",\"SPACE\"]\n",
    "    for word in most_common_words:\n",
    "        docs = nlp(word[0])\n",
    "        if docs[0].pos_==\"ADJ\" or docs[0].pos_==\"VERB\" or docs[0].pos_==\"ADV\":\n",
    "            word_features.append(docs[0].text)\n",
    "#         else:\n",
    "#             if docs[0].pos_ not in tags:\n",
    "#                 print(docs[0].text,docs[0].pos_)\n",
    "    return word_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents_feature_sets(documents,word_features):\n",
    "    balanced_documents=[]\n",
    "    for i in range(0,int(len(documents)/2)):\n",
    "            balanced_documents.append(documents[i])\n",
    "            balanced_documents.append(documents[i+int(len(documents)/2)])\n",
    "    featuresets = [(find_features(rev,documents,word_features), category) for (rev, category) in balanced_documents]\n",
    "    random.shuffle(featuresets)\n",
    "    if verbose==1:\n",
    "        print (len(featuresets[1][0].keys()))\n",
    "        balanced_documents[:2]\n",
    "    return featuresets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(document,documents,word_features):\n",
    "    words = set(document)\n",
    "    features_prob = {}\n",
    "    for w in word_features:\n",
    "        word_freq = document.count(w) / len(documents)\n",
    "        features_prob[w] = word_freq  ## compute frequency\n",
    "    return features_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(cm):\n",
    "    TP=cm._confusion[1][1]\n",
    "    FP=cm._confusion[1][0]\n",
    "    TN=cm._confusion[0][0]\n",
    "    FN=cm._confusion[0][1]\n",
    "\n",
    "    Recall=TP/(TP+FN)\n",
    "    Precision=TP/(TP+FP)\n",
    "    accuracy=(TP+TN)/(TP+TN+FP+FN)\n",
    "    F1score=2*(Recall * Precision) / (Recall + Precision)\n",
    "    if verbose==1:\n",
    "        print(\"Recall is - \",Recall)\n",
    "        print(\"Precision is - \",Precision)\n",
    "        print(\"accuracy is - \",accuracy)\n",
    "        print(\"F1score is - \",F1score)\n",
    "    return Recall,Precision,accuracy,F1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_Naive_Bayes_classification_system(word_features_number,all_words):\n",
    "    most_common_words=get_most_common_words(word_features_number,all_words)\n",
    "    word_features=feature_selector(most_common_words)\n",
    "    if verbose==1:\n",
    "        print (word_features[:100])\n",
    "        print(len(word_features))\n",
    "    feature_sets=get_documents_feature_sets(documents,word_features)\n",
    "    if verbose==1:\n",
    "        print(len(feature_sets))\n",
    "\n",
    "    # set that we'll train our classifier with\n",
    "    training_set_separator=int(len(feature_sets)*0.9)\n",
    "    training_set = feature_sets[:training_set_separator]\n",
    "    # set that we'll test against.\n",
    "    testing_set = feature_sets[training_set_separator:]\n",
    "\n",
    "    # In[18]:\n",
    "    classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "    testing_set_content=[i[0] for i in testing_set]\n",
    "    golden_label=[i[1] for i in testing_set]\n",
    "    tested_label=classifier.classify_many(testing_set_content)\n",
    "    cm = nltk.ConfusionMatrix(golden_label, tested_label)\n",
    "    if verbose==1:\n",
    "        print (cm)\n",
    "        print(\"Classifier accuracy percent:\", (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "        classifier.show_most_informative_features(50)\n",
    "\n",
    "    recall,precision,accuracy,F1score=calculate_metrics(cm)\n",
    "    all_words=[]\n",
    "    return [recall,precision,accuracy,F1score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_average_results(results):\n",
    "    average_recall=0\n",
    "    average_precision=0\n",
    "    average_accuracy=0\n",
    "    average_F1score=0\n",
    "    for result in results:\n",
    "        average_recall+=result[0]/len(results)\n",
    "        average_precision+=result[1]/len(results)\n",
    "        average_accuracy+=result[2]/len(results)\n",
    "        average_F1score+=result[3]/len(results)\n",
    "    return {\"Average recall\":\"{:.3%}\".format(average_recall), \"Average Precision\":\"{:.3%}\".format(average_precision), \"Average accuracy\":\"{:.3%}\".format(average_accuracy), \"Average F1 score\":\"{:.3%}\".format(average_F1score)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classification_system(tests, all_words,tests_number=5):\n",
    "    for t in tests:\n",
    "        i=0\n",
    "        av_results=[]\n",
    "        while i<tests_number:\n",
    "            av_results.append(run_Naive_Bayes_classification_system(t,all_words))    \n",
    "            i=i+1\n",
    "        print({str(t)+\" top-frequent words\":count_average_results(av_results), \"Tests quantity\":tests_number})\n",
    "           \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "(['happy', 'bastard', 'quick', 'movie', 'review', 'damn', 'bug', 'got', 'head', 'start', 'movie', 'starring', 'jamie', 'lee', 'curtis', 'baldwin', 'brother', 'william', 'time', 'story', 'crew', 'tugboat', 'come', 'deserted', 'russian', 'tech', 'ship', 'strangeness', 'kick', 'power', 'little', 'know', 'power', 'going', 'gore', 'bringing', 'action', 'sequence', 'virus', 'feel', 'like', 'movie', 'going', 'flash', 'substance', 'don', 'know', 'crew', 'middle', 'don', 'know', 'origin', 'took', 'ship', 'big', 'pink', 'flashy', 'thing', 'hit', 'mir', 'course', 'don', 'know', 'donald', 'sutherland', 'stumbling', 'drunkenly', 'hey', 'let', 'chase', 'people', 'robot', 'acting', 'average', 'like', 'curtis', 'likely', 'kick', 'work', 'halloween', 'sutherland', 'wasted', 'baldwin', 'acting', 'like', 'baldwin', 'course', 'real', 'star', 'stan', 'winston', 'robot', 'design', 'schnazzy', 'cgi', 'occasional', 'good', 'gore', 'shot', 'like', 'picking', 'brain', 'robot', 'body', 'turn', 'movie', 'pretty', 'sunken', 'ship', 'movie'], 'neg')\n"
     ]
    }
   ],
   "source": [
    "documents,words = create_documents()\n",
    "all_words=words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34261\n",
      "('film', 11053)\n",
      "('movie', 6977)\n",
      "('character', 3879)\n",
      "('like', 3789)\n",
      "('time', 2979)\n",
      "('scene', 2671)\n",
      "('good', 2429)\n",
      "('story', 2345)\n",
      "('life', 1913)\n",
      "('way', 1882)\n",
      "('year', 1732)\n",
      "('thing', 1661)\n",
      "('doe', 1578)\n",
      "('plot', 1574)\n",
      "('come', 1510)\n",
      "('little', 1505)\n",
      "('know', 1494)\n",
      "('people', 1470)\n",
      "('man', 1404)\n",
      "('bad', 1395)\n",
      "('work', 1379)\n",
      "('director', 1347)\n",
      "('best', 1334)\n",
      "('end', 1328)\n",
      "('performance', 1317)\n",
      "('don', 1304)\n",
      "('new', 1292)\n",
      "('look', 1278)\n",
      "('doesn', 1277)\n",
      "('action', 1260)\n",
      "('actor', 1252)\n",
      "('love', 1209)\n",
      "('play', 1205)\n",
      "('star', 1160)\n",
      "('role', 1155)\n",
      "('great', 1150)\n",
      "('find', 1119)\n",
      "('audience', 1079)\n",
      "('big', 1064)\n",
      "('world', 1061)\n",
      "('want', 1037)\n",
      "('day', 1024)\n",
      "('think', 986)\n",
      "('guy', 932)\n",
      "('comedy', 928)\n",
      "('better', 926)\n",
      "('real', 915)\n",
      "('seen', 910)\n",
      "('going', 901)\n",
      "('old', 887)\n",
      "('isn', 871)\n",
      "('fact', 853)\n",
      "('set', 851)\n",
      "('point', 848)\n",
      "('funny', 840)\n",
      "('actually', 837)\n",
      "('long', 836)\n",
      "('right', 834)\n",
      "('minute', 831)\n",
      "('woman', 831)\n",
      "('effect', 822)\n",
      "('lot', 814)\n",
      "('script', 810)\n",
      "('friend', 805)\n",
      "('john', 802)\n",
      "('played', 791)\n",
      "('cast', 790)\n",
      "('moment', 772)\n",
      "('turn', 766)\n",
      "('line', 755)\n",
      "('young', 743)\n",
      "('screen', 730)\n",
      "('original', 717)\n",
      "('place', 714)\n",
      "('feel', 701)\n",
      "('family', 700)\n",
      "('acting', 695)\n",
      "('problem', 689)\n",
      "('girl', 681)\n",
      "('try', 678)\n",
      "('picture', 677)\n",
      "('sequence', 659)\n",
      "('away', 655)\n",
      "('course', 650)\n",
      "('interesting', 638)\n",
      "('watch', 635)\n",
      "('far', 635)\n",
      "('high', 634)\n",
      "('start', 628)\n",
      "('bit', 617)\n",
      "('help', 614)\n",
      "('child', 610)\n",
      "('making', 609)\n",
      "('job', 609)\n",
      "('american', 608)\n",
      "('wife', 603)\n",
      "('alien', 603)\n",
      "('tell', 601)\n",
      "('kind', 600)\n",
      "('hour', 597)\n",
      "3000\n",
      "['good', 'come', 'little', 'know', 'bad', 'best', 'look', 'play', 'great', 'find', 'big', 'want', 'think', 'better', 'real', 'seen', 'going', 'old', 'funny', 'actually', 'long', 'played', 'turn', 'original', 'feel', 'acting', 'try', 'away', 'interesting', 'watch', 'far', 'high', 'start', 'making', 'tell', 'hard', 'begin', 'special', 'human', 'trying', 'instead', 'run', 'having', 'black', 'probably', 'pretty', 'sure', 'given', 'looking', 'let', 'watching', 'lead', 'fall', 'second', 'got', 'especially', 'ending', 'completely', 'different', 'small', 'simply', 'dead', 'left', 'true', 'lost', 'written', 'entire', 'getting', 'soon', 'main', 'found', 'fight', 'wrong', 'called', 'based', 'final', 'unfortunately', 'later', 'kill', 'playing', 'named', 'certainly', 'live', 'believe', 'said', 'finally', 'maybe', 'nice', 'perfect', 'seeing', 'able', 'open', 'won', 'including', 'stop', 'directed', 'running', 'supposed', 'worth', 'short']\n",
      "1051\n",
      "1051\n",
      "2000\n",
      "    |  n  p |\n",
      "    |  e  o |\n",
      "    |  g  s |\n",
      "----+-------+\n",
      "neg |<99> 9 |\n",
      "pos | 35<57>|\n",
      "----+-------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "Classifier accuracy percent: 78.0\n",
      "Most Informative Features\n",
      "               wonderful = 0.001             pos : neg    =     12.1 : 1.0\n",
      "                  stupid = 0.001             neg : pos    =     12.0 : 1.0\n",
      "                  boring = 0.001             neg : pos    =     11.6 : 1.0\n",
      "                terrific = 0.001             pos : neg    =     10.8 : 1.0\n",
      "             outstanding = 0.0005            pos : neg    =      9.9 : 1.0\n",
      "                     bad = 0.0025            neg : pos    =      9.8 : 1.0\n",
      "               ludicrous = 0.0005            neg : pos    =      9.5 : 1.0\n",
      "                   happy = 0.001             pos : neg    =      8.2 : 1.0\n",
      "              apparently = 0.001             neg : pos    =      7.9 : 1.0\n",
      "                     bad = 0.002             neg : pos    =      7.7 : 1.0\n",
      "             wonderfully = 0.0005            pos : neg    =      7.7 : 1.0\n",
      "                 private = 0.001             pos : neg    =      7.5 : 1.0\n",
      "                     bad = 0.003             neg : pos    =      7.1 : 1.0\n",
      "                   worst = 0.001             neg : pos    =      6.8 : 1.0\n",
      "                  finest = 0.0005            pos : neg    =      6.7 : 1.0\n",
      "                    best = 0.002             pos : neg    =      6.5 : 1.0\n",
      "                powerful = 0.001             pos : neg    =      6.5 : 1.0\n",
      "              ridiculous = 0.001             neg : pos    =      6.4 : 1.0\n",
      "                   great = 0.002             pos : neg    =      6.3 : 1.0\n",
      "                    dark = 0.0015            pos : neg    =      6.2 : 1.0\n",
      "                 similar = 0.001             pos : neg    =      6.2 : 1.0\n",
      "             magnificent = 0.0005            pos : neg    =      6.0 : 1.0\n",
      "                 idiotic = 0.0005            neg : pos    =      6.0 : 1.0\n",
      "                  wasted = 0.0005            neg : pos    =      5.9 : 1.0\n",
      "                   bland = 0.0005            neg : pos    =      5.9 : 1.0\n",
      "                  called = 0.0015            neg : pos    =      5.8 : 1.0\n",
      "               excellent = 0.001             pos : neg    =      5.7 : 1.0\n",
      "               different = 0.001             pos : neg    =      5.7 : 1.0\n",
      "                   awful = 0.0005            neg : pos    =      5.6 : 1.0\n",
      "                 clearly = 0.001             pos : neg    =      5.6 : 1.0\n",
      "                  normal = 0.001             pos : neg    =      5.6 : 1.0\n",
      "                 overall = 0.001             pos : neg    =      5.6 : 1.0\n",
      "             frightening = 0.001             pos : neg    =      5.6 : 1.0\n",
      "            surprisingly = 0.001             pos : neg    =      5.6 : 1.0\n",
      "                 average = 0.001             pos : neg    =      5.6 : 1.0\n",
      "              whatsoever = 0.0005            neg : pos    =      5.5 : 1.0\n",
      "                  giving = 0.001             pos : neg    =      5.3 : 1.0\n",
      "                supposed = 0.001             neg : pos    =      5.1 : 1.0\n",
      "                    stay = 0.001             neg : pos    =      5.1 : 1.0\n",
      "              uninspired = 0.0005            neg : pos    =      5.1 : 1.0\n",
      "             beautifully = 0.0005            pos : neg    =      5.1 : 1.0\n",
      "              ridiculous = 0.0005            neg : pos    =      4.9 : 1.0\n",
      "                    able = 0.0015            pos : neg    =      4.9 : 1.0\n",
      "                    lead = 0.0015            pos : neg    =      4.9 : 1.0\n",
      "                 leaving = 0.001             pos : neg    =      4.9 : 1.0\n",
      "              refreshing = 0.0005            pos : neg    =      4.9 : 1.0\n",
      "              constantly = 0.001             pos : neg    =      4.9 : 1.0\n",
      "              remarkable = 0.001             pos : neg    =      4.9 : 1.0\n",
      "                  handle = 0.001             pos : neg    =      4.9 : 1.0\n",
      "                 follows = 0.001             pos : neg    =      4.9 : 1.0\n",
      "Recall is -  0.8636363636363636\n",
      "Precision is -  0.6195652173913043\n",
      "accuracy is -  0.78\n",
      "F1score is -  0.7215189873417721\n",
      "{'3000 movie reviews': {'Average recall': '86.364%', 'Average Precision': '61.957%', 'Average accuracy': '78.000%', 'Average F1 score': '72.152%'}}\n"
     ]
    }
   ],
   "source": [
    "test_classification_system([3000],all_words,tests_number=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiments with different amount of top-frequent words for feature selection and training the Na—óve Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1000 top-frequent words': {'Average recall': '78.667%', 'Average Precision': '68.316%', 'Average accuracy': '75.400%', 'Average F1 score': '73.054%'}, 'Tests quantity': 5}\n",
      "{'2000 top-frequent words': {'Average recall': '79.350%', 'Average Precision': '76.105%', 'Average accuracy': '78.900%', 'Average F1 score': '77.578%'}, 'Tests quantity': 5}\n",
      "{'3000 top-frequent words': {'Average recall': '85.685%', 'Average Precision': '72.864%', 'Average accuracy': '79.700%', 'Average F1 score': '78.690%'}, 'Tests quantity': 5}\n",
      "{'4000 top-frequent words': {'Average recall': '79.783%', 'Average Precision': '73.343%', 'Average accuracy': '77.500%', 'Average F1 score': '76.409%'}, 'Tests quantity': 5}\n",
      "{'8000 top-frequent words': {'Average recall': '84.901%', 'Average Precision': '74.389%', 'Average accuracy': '80.800%', 'Average F1 score': '79.277%'}, 'Tests quantity': 5}\n",
      "{'10000 top-frequent words': {'Average recall': '83.237%', 'Average Precision': '72.804%', 'Average accuracy': '78.700%', 'Average F1 score': '77.582%'}, 'Tests quantity': 5}\n",
      "{'12000 top-frequent words': {'Average recall': '84.991%', 'Average Precision': '70.892%', 'Average accuracy': '78.400%', 'Average F1 score': '77.244%'}, 'Tests quantity': 5}\n",
      "{'15000 top-frequent words': {'Average recall': '82.020%', 'Average Precision': '71.394%', 'Average accuracy': '78.600%', 'Average F1 score': '76.285%'}, 'Tests quantity': 5}\n",
      "{'18000 top-frequent words': {'Average recall': '84.083%', 'Average Precision': '76.604%', 'Average accuracy': '80.900%', 'Average F1 score': '80.005%'}, 'Tests quantity': 5}\n",
      "{'20000 top-frequent words': {'Average recall': '82.572%', 'Average Precision': '72.826%', 'Average accuracy': '78.900%', 'Average F1 score': '77.317%'}, 'Tests quantity': 5}\n"
     ]
    }
   ],
   "source": [
    "verbose=0\n",
    "test_classification_system([1000,2000,3000,4000,8000,10000,12000, 15000, 18000, 20000],all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
